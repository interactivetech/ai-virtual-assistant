# Copyright (c) 2023, NVIDIA CORPORATION.  All rights reserved.
#
# NVIDIA CORPORATION and its licensors retain all intellectual property
# and proprietary rights in and to this software, related documentation
# and any modifications thereto.  Any use, reproduction, disclosure or
# distribution of this software and related documentation without an express
# license agreement from NVIDIA CORPORATION is strictly prohibited.

Component:
  app-version: 0.0.1
  description: default description
  helmUrlLocationPath: https://helm.ngc.nvidia.com/myorg/myteam/charts
  name: nemollm-inference
  version: 0.0.1
affinity: {}
applicationSpecs:
  nemollm-infer-deployment:
    apptype: stateless
    containers:
      nemollm-infer-container:
        env:
        - name: NGC_API_KEY
          value: ""
        image:
          repository: nvcr.io/nim/meta/llama-3.1-8b-instruct
          tag: latest
        command: ["/bin/bash", "-c"]#manually run: /bin/bash /opt/nim/start-server.sh
        args:
        - echo starting && sleep 800000;
        livenessProbe:
          exec:
            command:
            - curl
            - -f
            - http://localhost:8000/v1/health/ready
          failureThreshold: 100
          initialDelaySeconds: 10
          periodSeconds: 10
          timeoutSeconds: 20
        startupProbe:
          httpGet:
            path: /v1/health/ready
            port: 8000
          initialDelaySeconds: 40
          timeoutSeconds: 1
          periodSeconds: 10
          successThreshold: 1
          failureThreshold: 180
        readinessProbe:
          httpGet:
            path: /v1/health/ready
            port: 8000
          initialDelaySeconds: 15
          timeoutSeconds: 1
          periodSeconds: 10
          successThreshold: 1
          failureThreshold: 3
        ports:
        - containerPort: 8000
          name: http
        resources:
          requests:
            cpu: "32"
            memory: "150Gi"
            nvidia.com/gpu: 2
          limits:
            cpu: "32"
            memory: "150Gi"
            nvidia.com/gpu: 2
    securityContext:
      runAsGroup: 1000
      runAsUser: 1000
    services:
      llm-service:
        ports:
        - name: http-api
          port: 8000
          targetPort: 8000
defaultVolumeMounts:
- mountPath: /opt/workload-config
  name: workload-cm-volume
- mountPath: /opt/configs
  name: configs-volume
- mountPath: /opt/scripts
  name: scripts-cm-volume
defaultVolumes:
- configMap:
    name: nemollm-inference-workload-cm
  name: workload-cm-volume
- configMap:
    name: nemollm-inference-configs-cm
  name: configs-volume
- configMap:
    name: nemollm-inference-scripts-cm
  name: scripts-cm-volume
egress: {}
externalFiles: []
image:
  pullPolicy: IfNotPresent
# imagePullSecrets:
# - name: ngc-docker-reg-secret
ingress:
  enabled: false
metrics: {}
nodeSelector: {}
podAnnotations: {}
podSecurityContext: {}
replicaCount: 1
resources: {}
securityContext: {}
storageClaims: {}
tolerations: []
workloadSpecs:
  dummy: {}



configs:
  config.yaml:
    SampleConfig:
      sampleValue: 0

